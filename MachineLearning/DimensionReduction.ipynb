{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitdataconda5467872f862141fb83a5467f0628293e",
   "display_name": "Python 3.8.1 64-bit ('Data': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ML Models \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Model Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set pyplot style\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset from datasets directory\n",
    "ansur = pd.read_csv('~/DevSpace/Data-Snippets/MachineLearning/datasets/ansur.csv')\n",
    "ansur.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot perfect correlation in data\n",
    "cmap = sns.diverging_palette(h_neg=10, h_pos=240, as_cmap=True)\n",
    "sns.heatmap(ansur.corr(), center=0, cmap=cmap, linewidths=1, annot=True, fmt='.2f')\n",
    "plt.title('Correlation Matrix for Ansur Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute force removal of hightly correlated features\n",
    "abs_corr_matrix = ansur.corr().abs()\n",
    "\n",
    "# Upper triagule of corr matrix to false\n",
    "mask = np.triu(np.ones_like(abs_corr_matrix, dtype=bool))\n",
    "tri_df = abs_corr_matrix.mask(mask)\n",
    "\n",
    "# Filter highly correlated dimensions\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]\n",
    "\n",
    "# Drop highly correlated dimensions\n",
    "reduced_df = ansur.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check visualization\n",
    "sns.pairplot(reduced_df, hue='Gender', diag_kind='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High dimensional data import\n",
    "body_dims = pd.read_csv('~/DevSpace/Data-Snippets/MachineLearning/datasets/body_measurements.csv')\n",
    "body_dims.drop('ID', axis=1, inplace=True)\n",
    "print(f'Number of dimensions (cols): {len(body_dims.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate TSNE Model\n",
    "tsne_model = TSNE(learning_rate=50)\n",
    "\n",
    "# Fit and transform numeric data from 4th column\n",
    "tsne_features = tsne_model.fit_transform(body_dims.iloc[:, 4:])\n",
    "\n",
    "# Add components to df\n",
    "body_dims['x'] = tsne_features[:, 0]\n",
    "body_dims['y'] = tsne_features[:, 1]\n",
    "\n",
    "# Plot transformed data\n",
    "sns.scatterplot(x='x', y='y', hue='Gender', data=body_dims)\n",
    "plt.xlabel('X Dimension')\n",
    "plt.ylabel('Y Dimension')\n",
    "plt.title('t-SNE of Body Measurements')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict 'Gender' based on all body measurements\n",
    "y = body_dims['Gender']\n",
    "X = body_dims.drop(['Branch', 'Component', 'Gender', 'x', 'y'], axis=1)\n",
    "\n",
    "# Split train, test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=12)\n",
    "\n",
    "# Instanciate model and fit model\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Measure accuracy\n",
    "acc_train = accuracy_score(y_train, svm.predict(X_train))\n",
    "acc_test = accuracy_score(y_test, svm.predict(X_test))\n",
    "\n",
    "# Measure overfitting abs(train - test / test)\n",
    "print(f'{acc_train = }')\n",
    "print(f'{acc_test = }')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Gender based on 'neckcircumferencebase' feature \n",
    "y = body_dims['Gender']\n",
    "X = body_dims[['neckcircumferencebase']]\n",
    "\n",
    "# Split train, test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=12)\n",
    "\n",
    "# Instanciate model and fit model\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Measure accuracy\n",
    "acc_train = accuracy_score(y_train, svm.predict(X_train))\n",
    "acc_test = accuracy_score(y_test, svm.predict(X_test))\n",
    "\n",
    "# Measure overfitting abs(train - test / test)\n",
    "print(f'{acc_train = }')\n",
    "print(f'{acc_test = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the distribution of the variances from all dimensions from body_dims\n",
    "\n",
    "# Normalize columns for better visualization\n",
    "numeric_dims = body_dims.iloc[:, 3:-2] / body_dims.iloc[:, 3:-2].mean()\n",
    "\n",
    "# Calculate variances, median and mean\n",
    "vars = numeric_dims.std() ** 2\n",
    "median = round(vars.quantile(), 5)\n",
    "mean = round(vars.mean(), 5)\n",
    "\n",
    "# Plot variance distribution and annotate\n",
    "sns.boxplot(y=vars.values)\n",
    "plt.annotate(f'Median variance: {median}', (-0.45, 0.022))\n",
    "plt.annotate(f'Mean variance: {mean}', (-0.45, 0.0195))\n",
    "\n",
    "plt.xlabel('Variances')\n",
    "plt.ylabel('Normalized Variance')\n",
    "plt.title('Normalized variance distribution of dimensions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality with a variance threshold (median variance)\n",
    "variance_selection = VarianceThreshold(threshold=0.00482)\n",
    "\n",
    "# Fit variance selector\n",
    "variance_selection.fit(numeric_dims)\n",
    "\n",
    "# Get variance mask\n",
    "mask = variance_selection.get_support()\n",
    "\n",
    "# Reduced body_dims\n",
    "reduced_body = numeric_dims.loc[:, mask]\n",
    "\n",
    "# New number of dimensions\n",
    "print(f'Old dimensions: {len(body_dims.columns)}')\n",
    "print(f'New dimensions: {len(reduced_body.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Gender based on reduced dataset\n",
    "y = body_dims['Gender']\n",
    "X = reduced_body\n",
    "\n",
    "# Split train, test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=12)\n",
    "\n",
    "# Instanciate model and fit model\n",
    "svm = SVC(C=0.1)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Measure accuracy\n",
    "acc_train = accuracy_score(y_train, svm.predict(X_train))\n",
    "acc_test = accuracy_score(y_test, svm.predict(X_test))\n",
    "\n",
    "# Measure overfitting abs(train - test / test)\n",
    "print(f'{acc_train = }')\n",
    "print(f'{acc_test = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Gender with a classifier less prone to overfit (simpler model)\n",
    "log_reg = LogisticRegression(solver='newton-cg', C=0.1)\n",
    "\n",
    "# Fit Model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Measure Accuracy\n",
    "acc_train = accuracy_score(y_train, log_reg.predict(X_train))\n",
    "acc_test = accuracy_score(y_test, log_reg.predict(X_test))\n",
    "\n",
    "# Measure overfitting\n",
    "print(f'{acc_train = }')\n",
    "print(f'{acc_test = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of dimensions and coefficients\n",
    "lr_coefs = dict(zip(reduced_body.columns, np.abs(log_reg.coef_[0])))\n",
    "\n",
    "# Sort by most important coefficients\n",
    "lr_coefs = sorted(lr_coefs.items(), key=lambda x: x[1], reverse=True)\n",
    "pd.Series(dict(lr_coefs[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate and fit a Recursive Feature Elimination\n",
    "rfe = RFE(estimator=log_reg, n_features_to_select=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Sort ranked features by incresing importance\n",
    "feature_ranking = dict(zip(X.columns, rfe.ranking_))\n",
    "sorted_features = dict(sorted(feature_ranking.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Order DataFrame and set X, y\n",
    "ranking_df = reduced_body[sorted_features.keys()]\n",
    "X, y = ranking_df, body_dims['Gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Iterate over ordered columns and test accuracy of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save accuracy scores\n",
    "accuracy = []\n",
    "\n",
    "# Iterate over all models\n",
    "for i in range(len(sorted_features)):\n",
    "\n",
    "    # Train Test Split with ordered features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.iloc[:, i:], y, train_size=0.7, random_state=12)\n",
    "\n",
    "    # Train model with those features\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Measure accuracy\n",
    "    train_acc = accuracy_score(y_train, log_reg.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, log_reg.predict(X_test))\n",
    "    accuracy.append([len(X_train.columns), train_acc, test_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot accuracy scores for every RFE selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract accuracy data from experiment\n",
    "accuracy = np.array(accuracy)\n",
    "features, train, test = np.split(accuracy, 3, axis=1)\n",
    "features = features.flatten().astype('int')\n",
    "\n",
    "# Plot lines of training adn testing accuracy\n",
    "plt.plot(train, label='Train Score')\n",
    "plt.plot(test, label='Test Score')\n",
    "plt.xticks(features[::5], features[::-5])\n",
    "\n",
    "# Annotate figure\n",
    "plt.title('Train & Test scores by number of features')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Number of Features')\n",
    "\n",
    "# Figure style and legend\n",
    "plt.xlim(0, 45)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dimensionality reduction with Decision Tree Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import High Dimensional Data\n",
    "diabetes = pd.read_csv('~/DevSpace/Data-Snippets/MachineLearning/datasets/diabetes.csv', index_col='index')\n",
    "\n",
    "\n",
    "# Set X and y\n",
    "X = diabetes.drop(['diagnostic'], axis=1)\n",
    "y = diabetes['diagnostic']\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75)\n",
    "\n",
    "# Feature selection with Tree Based Models\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Measure Accuracy\n",
    "acc_train = accuracy_score(y_train, rf.predict(X_train))\n",
    "acc_test = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Sort important features\n",
    "features = pd.Series(data=rf.feature_importances_.round(2), index=X.columns)\n",
    "print(features.sort_values(ascending=False), '\\n')\n",
    "\n",
    "# Measure accuracy\n",
    "print(f'{acc_train = }')\n",
    "print(f'{acc_test = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dimensionality reduction with Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X and y\n",
    "X = diabetes.drop(['diagnostic', 'bmi'], axis=1)\n",
    "y = diabetes['bmi']\n",
    "\n",
    "# Train Test Split and scale data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create the Lasso model and fit to data\n",
    "la = Lasso(alpha=0.05)\n",
    "la.fit(X_train_std, y_train)\n",
    "\n",
    "# Transform the test set with the pre-fitted scaler\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "print(f\"Model predicted {r_squared:.2%} of the variance.\")\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "n_ignored = sum(zero_coef)\n",
    "print(f\"The model has ignored {n_ignored} out of {len(la.coef_)} features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "# Create and fit the LassoCV model on the training set\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
    "\n",
    "# Calculate R squared on the test set\n",
    "r_squared = lcv.score(X_test, y_test)\n",
    "print(f'The model explains {r_squared:.1%} of the test set variance')\n",
    "\n",
    "# Create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "print(f'{sum(lcv_mask)} features out of {len(lcv_mask)} selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
    "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
    "             n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_gb.score(X_test, y_test)\n",
    "print(f'The model can explain {r_squared:.1%} of the variance in the test set')\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "gb_mask = rfe_gb.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
    "             n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "rf_mask = rfe_rf.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the votes of the three models\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "\n",
    "# Create a mask for features selected by all 3 models\n",
    "meta_mask = votes >= 3\n",
    "\n",
    "# Apply the dimensionality reduction on X\n",
    "X_reduced = X.loc[:, meta_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Linear Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create model\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Measure accucary R^2\n",
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "\n",
    "# Result\n",
    "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Import dataset\n",
    "ansur = pd.read_csv('~/DevSpace/Data-Snippets/MachineLearning/datasets/ansur.csv')\n",
    "ansur_df = ansur.drop(['Gender', 'Branch', 'Component', 'ID'], axis=1)\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "n_components = 10\n",
    "\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "pca = PCA(n_components=n_components)\n",
    "pc = pca.fit_transform(ansur_std)\n",
    "pc_df = pd.DataFrame(pc, columns=[f'PC_{x}' for x in range(1, n_components + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot of the first n principal components\n",
    "sns.pairplot(pc_df.iloc[:, :4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variance explanation by component\n",
    "plt.plot(range(n_components), pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "# Annotate plot\n",
    "plt.title('Explained variance by PCA')\n",
    "plt.xlabel('Total components')\n",
    "plt.ylabel('Total variance explained')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}